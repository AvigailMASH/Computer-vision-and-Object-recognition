{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hey8xu47ECA7"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  models\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QnixgvDvEJwv"
   },
   "outputs": [],
   "source": [
    "#data.py\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# once the images are loaded, how do we pre-process them before being passed into the network\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        #data augmentation\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #size of images from pretrained models\n",
    "        transforms.CenterCrop(size=224),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]) \n",
    "    ]),\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2ana_5SER2S",
    "outputId": "ec4bd614-65c1-48ab-d0dd-ebd254ec0067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42,541,140 total parameters.\n",
      "41,096,212 learnable parameters.\n"
     ]
    }
   ],
   "source": [
    "#model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nclasses = 20 \n",
    "\n",
    "#choice of the pretrained model\n",
    "model = models.resnet101(pretrained=True)\n",
    "#adpating the size of the fully connected layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, nclasses)\n",
    "\n",
    "#We want to finetune on lasts layers and freeze the first layers, that do not contain specific features for our task.\n",
    "#That's why we first need to know the architecture of the model we chose. \n",
    "#As we can see, there are four stages of convolution and one fully connected layer.\n",
    "#model.eval()\n",
    "\n",
    "#An idea would be to freeze the three first stages of convolution and finetune on the last stage of convolution and one the fc layer.\n",
    "#Rq: by default, requires_grad=True, i.e unfrozen (or learnable).\n",
    "learnable_params = []\n",
    "for name, module in model.named_children():\n",
    "    if name not in ['layer3','layer4','fc']:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False  #freeze\n",
    "    if name in ['layer3','layer4','fc']:\n",
    "        for param in module.parameters():\n",
    "            learnable_params.append(param)\n",
    "\n",
    "\n",
    "#Let's check the number of parameters we will train.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} learnable parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XwkaEZvZESzW",
    "outputId": "9c0c3537-012b-41ae-d21e-f036d37721ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oHFvTI5sFVV5",
    "outputId": "19360f2c-f566-4906-d8f2-2b9831928b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "#check GPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRRUUybeEdqY",
    "outputId": "3d6ed7d2-d339-4eea-9a98-b099f6bd2b8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Train Epoch: 1 [0/1082 (0%)]\tLoss: 3.010761\n",
      "Train Epoch: 1 [640/1082 (59%)]\tLoss: 1.987398\n",
      "\n",
      "Validation set: Average loss: 0.0147, Accuracy: 76/103 (74%)\n",
      "Saved model to experiment/model_1.pth. You can run `python evaluate.py --model experiment/model_1.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 2 [0/1082 (0%)]\tLoss: 0.915224\n",
      "Train Epoch: 2 [640/1082 (59%)]\tLoss: 0.393403\n",
      "\n",
      "Validation set: Average loss: 0.0105, Accuracy: 86/103 (83%)\n",
      "Saved model to experiment/model_2.pth. You can run `python evaluate.py --model experiment/model_2.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 3 [0/1082 (0%)]\tLoss: 0.207317\n",
      "Train Epoch: 3 [640/1082 (59%)]\tLoss: 0.218694\n",
      "\n",
      "Validation set: Average loss: 0.0086, Accuracy: 89/103 (86%)\n",
      "Saved model to experiment/model_3.pth. You can run `python evaluate.py --model experiment/model_3.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 4 [0/1082 (0%)]\tLoss: 0.088526\n",
      "Train Epoch: 4 [640/1082 (59%)]\tLoss: 0.026582\n",
      "\n",
      "Validation set: Average loss: 0.0128, Accuracy: 86/103 (83%)\n",
      "Saved model to experiment/model_4.pth. You can run `python evaluate.py --model experiment/model_4.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 5 [0/1082 (0%)]\tLoss: 0.076059\n",
      "Train Epoch: 5 [640/1082 (59%)]\tLoss: 0.045699\n",
      "\n",
      "Validation set: Average loss: 0.0073, Accuracy: 88/103 (85%)\n",
      "Saved model to experiment/model_5.pth. You can run `python evaluate.py --model experiment/model_5.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 6 [0/1082 (0%)]\tLoss: 0.038210\n",
      "Train Epoch: 6 [640/1082 (59%)]\tLoss: 0.058822\n",
      "\n",
      "Validation set: Average loss: 0.0094, Accuracy: 89/103 (86%)\n",
      "Saved model to experiment/model_6.pth. You can run `python evaluate.py --model experiment/model_6.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 7 [0/1082 (0%)]\tLoss: 0.014560\n",
      "Train Epoch: 7 [640/1082 (59%)]\tLoss: 0.063521\n",
      "\n",
      "Validation set: Average loss: 0.0089, Accuracy: 91/103 (88%)\n",
      "Saved model to experiment/model_7.pth. You can run `python evaluate.py --model experiment/model_7.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 8 [0/1082 (0%)]\tLoss: 0.019561\n",
      "Train Epoch: 8 [640/1082 (59%)]\tLoss: 0.012664\n",
      "\n",
      "Validation set: Average loss: 0.0074, Accuracy: 92/103 (89%)\n",
      "Saved model to experiment/model_8.pth. You can run `python evaluate.py --model experiment/model_8.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 9 [0/1082 (0%)]\tLoss: 0.005426\n",
      "Train Epoch: 9 [640/1082 (59%)]\tLoss: 0.002432\n",
      "\n",
      "Validation set: Average loss: 0.0086, Accuracy: 92/103 (89%)\n",
      "Saved model to experiment/model_9.pth. You can run `python evaluate.py --model experiment/model_9.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 10 [0/1082 (0%)]\tLoss: 0.004845\n",
      "Train Epoch: 10 [640/1082 (59%)]\tLoss: 0.010409\n",
      "\n",
      "Validation set: Average loss: 0.0082, Accuracy: 92/103 (89%)\n",
      "Saved model to experiment/model_10.pth. You can run `python evaluate.py --model experiment/model_10.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 11 [0/1082 (0%)]\tLoss: 0.002939\n",
      "Train Epoch: 11 [640/1082 (59%)]\tLoss: 0.002279\n",
      "\n",
      "Validation set: Average loss: 0.0082, Accuracy: 93/103 (90%)\n",
      "Saved model to experiment/model_11.pth. You can run `python evaluate.py --model experiment/model_11.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 12 [0/1082 (0%)]\tLoss: 0.005592\n",
      "Train Epoch: 12 [640/1082 (59%)]\tLoss: 0.015490\n",
      "\n",
      "Validation set: Average loss: 0.0082, Accuracy: 94/103 (91%)\n",
      "Saved model to experiment/model_12.pth. You can run `python evaluate.py --model experiment/model_12.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 13 [0/1082 (0%)]\tLoss: 0.001785\n",
      "Train Epoch: 13 [640/1082 (59%)]\tLoss: 0.003294\n",
      "\n",
      "Validation set: Average loss: 0.0090, Accuracy: 93/103 (90%)\n",
      "Saved model to experiment/model_13.pth. You can run `python evaluate.py --model experiment/model_13.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 14 [0/1082 (0%)]\tLoss: 0.006936\n",
      "Train Epoch: 14 [640/1082 (59%)]\tLoss: 0.004338\n",
      "\n",
      "Validation set: Average loss: 0.0092, Accuracy: 93/103 (90%)\n",
      "Saved model to experiment/model_14.pth. You can run `python evaluate.py --model experiment/model_14.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 15 [0/1082 (0%)]\tLoss: 0.006521\n",
      "Train Epoch: 15 [640/1082 (59%)]\tLoss: 0.003604\n",
      "\n",
      "Validation set: Average loss: 0.0079, Accuracy: 94/103 (91%)\n",
      "Saved model to experiment/model_15.pth. You can run `python evaluate.py --model experiment/model_15.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 16 [0/1082 (0%)]\tLoss: 0.002099\n",
      "Train Epoch: 16 [640/1082 (59%)]\tLoss: 0.029715\n",
      "\n",
      "Validation set: Average loss: 0.0076, Accuracy: 96/103 (93%)\n",
      "Saved model to experiment/model_16.pth. You can run `python evaluate.py --model experiment/model_16.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 17 [0/1082 (0%)]\tLoss: 0.019613\n",
      "Train Epoch: 17 [640/1082 (59%)]\tLoss: 0.041486\n",
      "\n",
      "Validation set: Average loss: 0.0111, Accuracy: 90/103 (87%)\n",
      "Saved model to experiment/model_17.pth. You can run `python evaluate.py --model experiment/model_17.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 18 [0/1082 (0%)]\tLoss: 0.002918\n",
      "Train Epoch: 18 [640/1082 (59%)]\tLoss: 0.002483\n",
      "\n",
      "Validation set: Average loss: 0.0069, Accuracy: 93/103 (90%)\n",
      "Saved model to experiment/model_18.pth. You can run `python evaluate.py --model experiment/model_18.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 19 [0/1082 (0%)]\tLoss: 0.002097\n",
      "Train Epoch: 19 [640/1082 (59%)]\tLoss: 0.004526\n",
      "\n",
      "Validation set: Average loss: 0.0077, Accuracy: 93/103 (90%)\n",
      "Saved model to experiment/model_19.pth. You can run `python evaluate.py --model experiment/model_19.pth` to generate the Kaggle formatted csv file\n",
      "\n",
      "Train Epoch: 20 [0/1082 (0%)]\tLoss: 0.002120\n",
      "Train Epoch: 20 [640/1082 (59%)]\tLoss: 0.002424\n",
      "\n",
      "Validation set: Average loss: 0.0087, Accuracy: 94/103 (91%)\n",
      "Saved model to experiment/model_20.pth. You can run `python evaluate.py --model experiment/model_20.pth` to generate the Kaggle formatted csv file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#choice of some parameters\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='RecVis A3 training script')\n",
    "parser.add_argument('--data', type=str, default='/content/drive/My Drive/bird_dataset/bird_dataset', metavar='D',\n",
    "                    help=\"folder where data is located. train_images/ and val_images/ need to be found in the folder\")\n",
    "parser.add_argument('--batch-size', type=int, default=64, metavar='B',\n",
    "                    help='input batch size for training (default: 64)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                    help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                    help='SGD momentum (default: 0.5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--experiment', type=str, default='experiment', metavar='E',\n",
    "                    help='folder where experiment outputs are located.')\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Create experiment folder\n",
    "if not os.path.isdir(args.experiment):\n",
    "    os.makedirs(args.experiment)\n",
    "\n",
    "# Data initialization and loading\n",
    "#from data import data_transforms\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(args.data + '/train_images',\n",
    "                         transform=data_transforms['train']),\n",
    "    batch_size=args.batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(args.data + '/val_images',\n",
    "                         transform=data_transforms['valid']),\n",
    "    batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "# Neural network and optimizer\n",
    "# We define neural net in model.py so that it can be reused by the evaluate.py script\n",
    "#from model import Net \n",
    "\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "optimizer = optim.SGD(learnable_params, lr=0.01, momentum=0.9)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in val_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "        validation_loss += criterion(output, target).data.item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        validation_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    validation()\n",
    "    model_file = args.experiment + '/model_' + str(epoch) + '.pth'\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "    print('Saved model to ' + model_file + '. You can run `python evaluate.py --model ' + model_file + '` to generate the Kaggle formatted csv file\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JYNSeBcZme3",
    "outputId": "cb026cb0-d0af-4313-e388-4ef2656e01ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/517 [00:00<00:12, 40.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 517/517 [00:13<00:00, 38.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully wrote experiment/kaggle.csv, you can upload this file to the kaggle competition website\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "\n",
    "import torch\n",
    "\n",
    "#from model import Net\n",
    "\n",
    "parser = argparse.ArgumentParser(description='RecVis A3 evaluation script')\n",
    "parser.add_argument('--data', type=str, default='/content/drive/My Drive/bird_dataset/bird_dataset', metavar='D',\n",
    "                    help=\"folder where data is located. test_images/ need to be found in the folder\")\n",
    "parser.add_argument('--model', type=str, metavar='M',\n",
    "                    help=\"the model file to be evaluated. Usually it is of the form model_X.pth\")\n",
    "parser.add_argument('--outfile', type=str, default='experiment/kaggle.csv', metavar='D',\n",
    "                    help=\"name of the output csv file\")\n",
    "parser.add_argument('--experiment', type=str, default='experiment', metavar='E',\n",
    "                    help='folder where experiment outputs are located.')\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "state_dict = torch.load(args.experiment+'/model_20.pth')\n",
    "#model_file=args.experiment+'\\model_'+str(epoch)+'.pth'\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "if use_cuda:\n",
    "    print('Using GPU')\n",
    "    model.cuda()\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "#from data import data_transforms\n",
    "\n",
    "test_dir = args.data + '/test_images/mistery_category'\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "\n",
    "output_file = open(args.outfile, \"w\")\n",
    "output_file.write(\"Id,Category\\n\")\n",
    "\n",
    "for f in tqdm(os.listdir(test_dir)):\n",
    "    if 'jpg' in f:\n",
    "        data = data_transforms['test'](pil_loader(test_dir + '/' + f))\n",
    "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
    "\n",
    "output_file.close()\n",
    "\n",
    "print(\"Succesfully wrote \" + args.outfile + ', you can upload this file to the kaggle competition website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuV3NodMy_4r"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A3_modif1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
